# -*- coding: utf-8 -*-
"""sentiment_analysis.py

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1buUSZqht2SyMw0_3GGoWMVQmviKZPrr9
"""

# -*- coding: utf-8 -*-
"""
Created on Thu Jun 23 09:16:10 2022

@author: User
"""

import os
import re
import json
import pickle
import datetime
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from tensorflow.keras.utils import plot_model
from sklearn.preprocessing import OneHotEncoder
from tensorflow.keras.callbacks import TensorBoard
from sklearn.metrics import ConfusionMatrixDisplay
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

from modules_for_sentiment import ModelCreation, ModelEvaluation

#%% Statics

CSV_URL = "https://raw.githubusercontent.com/susanli2016/PyCon-Canada-2019-NLP-Tutorial/master/bbc-text.csv"
log_dir = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
LOG_FOLDER_PATH = os.path.join(os.getcwd(),'segmentation_logs', log_dir)
MODEL_SAVE_PATH = os.path.join(os.getcwd(),'saved_models','model.h5')
OHE_PATH = os.path.join(os.getcwd(),'saved_models','ohe.pkl')
TOKENIZER_PATH = os.path.join(os.getcwd(),'saved_models','tokenizer_sentiment.json')

#%% Step 1) Data Loading

df = pd.read_csv(CSV_URL)

#%% Step 2) Data Inspection

df.head()
df.tail()

df.info()
df.describe()

df['category'].unique() # to get the unique target values
df['text'][0]
df['category'][0]

df.duplicated().sum() # 99 duplicated data

#%% Step 3) Data Cleaning

df = df.drop_duplicates()

text = df['text'].values
category = df['category'].values

# The words are already in lower cases. But just in case if there are any 
# unseen capital letters, we do a lower() function.
# Remove numbers as well, and split the words.

for index,rev in enumerate(text):
    # convert into lower case
    # remove numbers
    text[index] = re.sub(' \d+',' ',rev).lower().split()

#%% Step 4) Features Selection

# Nothing to select

#%% Step 5) Data Preprocessing

# 1. Convert to lowercase

# Already done it

# 2. Tokenization

vocab_size = 10000
oov_token = 'OOV'

tokenizer = Tokenizer(num_words=vocab_size,oov_token=oov_token)

tokenizer.fit_on_texts(text) # to learn all of the words
word_index = tokenizer.word_index
print(word_index)

train_sequences = tokenizer.texts_to_sequences(text) # to convert into numbers

# 3. Padding & Truncating

length_of_review = [len(i) for i in train_sequences] # list comprehension
np.median(length_of_review) # to get the number of max length for padding

max_len = 340

padded_review = pad_sequences(train_sequences,maxlen=max_len,
                              padding='post',
                              truncating='post')

num_class = len(np.unique(category))

# 4. One Hot Encoding

ohe = OneHotEncoder(sparse=False)
category = ohe.fit_transform(np.expand_dims(category,axis=-1))

# 5. Train test split

X_train,X_test,y_train,y_test = train_test_split(padded_review,category,
                                                 test_size=0.3,
                                                 random_state=123)

X_train = np.expand_dims(X_train,axis=-1)
X_test = np.expand_dims(X_test,axis=-1)


#%% Model Development

embedding_dim = 64
num_feature = np.shape(X_train)[1]

mc = ModelCreation()

model = mc.nlp_model(num_feature,num_class,vocab_size,embedding_dim)

model.summary()

tensorboard_callback = TensorBoard(log_dir=LOG_FOLDER_PATH)

model.compile(loss='categorical_crossentropy',optimizer='adam',metrics='acc')

hist = model.fit(x=X_train,y=y_train,batch_size=64,epochs=50,
                 validation_data=(X_test,y_test),
                 callbacks=[tensorboard_callback])

hist.history.keys()
training_loss = hist.history['loss']
training_acc = hist.history['acc']
validation_acc = hist.history['val_acc']
validation_loss = hist.history['val_loss']

# Graph plot
mc.result_plot(training_loss,validation_loss,training_acc,validation_acc)

#%% Savings

model.save(MODEL_SAVE_PATH)

token_json = tokenizer.to_json()

with open(TOKENIZER_PATH,'w') as file:
    json.dump(token_json,file)

with open(OHE_PATH,'wb') as file:
    pickle.dump(ohe,file)

#%% Plot model

plot_model(model,show_shapes=True,show_layer_names=(True))

#%% Model Evaluation

results = model.evaluate(X_test,y_test)
print(results)

y_pred = np.argmax(model.predict(X_test), axis=1)
y_true = np.argmax(y_test,axis=1)

me = ModelEvaluation()

me.scores(y_true,y_pred)

cm = confusion_matrix(y_true,y_pred)

disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap=plt.cm.Blues)
plt.show()

# Commented out IPython magic to ensure Python compatibility.
# %reload_ext tensorboard
# %tensorboard --logdir segmentation_logs

#%% Discussion

# The model scores an accuracy of 79%. Although is it overfitting, the training
# accuracy scores 98%, the model is quite good in my opinion.
# The model consist of embedding, LSTM, Bidirectional and dense layers.
# Dropout layers of 30% are also included to avoid high overfitting.






